{"author":"tbrandenburg","body":"@task Embed the following python file in an uv project and test it. I want to run it via \"uv run pyrag\". No other commandline parameters or environment variables.\nApply a proper RAG architecture, but without adding features - keep it simple as it is - only restructure into loader, chunker, embedder, storage and search for clarity.\nDo not use an HuggingFace token, instead use all-MiniLM-L6-v2 sentence transformer.\nApply PEP8 and run uv ruff check and uv ruff format.\n\n#!/usr/bin/env python3\n\"\"\"\nDocling RAG with LangChain Example\nConverted from: https://github.com/docling-project/docling/blob/main/docs/examples/rag_langchain.ipynb\n\nThis script demonstrates a complete RAG pipeline using:\n- Docling for document processing\n- LangChain for document loading and chain creation\n- Milvus for vector storage\n- HuggingFace for embeddings and LLM\n\"\"\"\n\nimport os\nimport json\nfrom pathlib import Path\nfrom tempfile import mkdtemp\nfrom dotenv import load_dotenv\n\nfrom langchain_core.prompts import PromptTemplate\nfrom langchain_docling import DoclingLoader\nfrom langchain_docling.loader import ExportType\nfrom docling.chunking import HybridChunker\nfrom langchain_text_splitters import MarkdownHeaderTextSplitter\nfrom langchain_huggingface.embeddings import HuggingFaceEmbeddings\nfrom langchain_milvus import Milvus\nfrom langchain.chains import create_retrieval_chain\nfrom langchain.chains.combine_documents import create_stuff_documents_chain\nfrom langchain_huggingface import HuggingFaceEndpoint\n\n\ndef _get_env_from_colab_or_os(key):\n    \"\"\"Get environment variable from Colab secrets or OS environment.\"\"\"\n    try:\n        from google.colab import userdata\n        try:\n            return userdata.get(key)\n        except userdata.SecretNotFoundError:\n            pass\n    except ImportError:\n        pass\n    return os.environ.get(key, \"\")\n\n\ndef clip_text(text, threshold=100):\n    \"\"\"Clip text to a maximum length.\"\"\"\n    return f\"{text[:threshold]}...\" if len(text) \u003e threshold else text\n\n\ndef main():\n    # Load environment variables\n    load_dotenv()\n    \n    # Configuration\n    FILE_PATH = \"https://arxiv.org/pdf/2408.09869\"  # Docling Technical Report\n    EXPORT_TYPE = ExportType.DOC_CHUNKS  # Options: ExportType.DOC_CHUNKS or ExportType.MARKDOWN\n    EMBED_MODEL_ID = \"sentence-transformers/all-MiniLM-L6-v2\"\n    GEN_MODEL_ID = \"mistralai/Mistral-7B-Instruct-v0.2\"\n    TOP_K = 5\n    QUESTION = \"Which are the main AI models in Docling?\"\n    \n    # Get HuggingFace token\n    HF_TOKEN = _get_env_from_colab_or_os(\"HF_TOKEN\")\n    if not HF_TOKEN:\n        print(\"Warning: HF_TOKEN not found. Set it in .env file or environment.\")\n    \n    # Define prompt template\n    PROMPT = PromptTemplate.from_template(\n        \"\"\"You are a helpful assistant. Answer the following question based on the provided context.\n\nContext:\n{context}\n\nQuestion: {input}\n\nAnswer:\"\"\"\n    )\n    \n    print(\"=\" * 80)\n    print(\"Docling RAG with LangChain Pipeline\")\n    print(\"=\" * 80)\n    \n    # Step 1: Load documents with Docling\n    print(f\"\\n1. Loading document from: {FILE_PATH}\")\n    print(f\"   Export type: {EXPORT_TYPE}\")\n    \n    loader = DoclingLoader(\n        file_path=FILE_PATH,\n        export_type=EXPORT_TYPE,\n        chunker=HybridChunker(tokenizer=EMBED_MODEL_ID),\n    )\n    docs = loader.load()\n    print(f\"   Loaded {len(docs)} document(s)\")\n    \n    # Step 2: Split documents based on export type\n    print(\"\\n2. Splitting documents...\")\n    \n    if EXPORT_TYPE == ExportType.DOC_CHUNKS:\n        splits = docs\n        print(f\"   Using doc chunks: {len(splits)} chunks\")\n    elif EXPORT_TYPE == ExportType.MARKDOWN:\n        splitter = MarkdownHeaderTextSplitter(\n            headers_to_split_on=[\n                (\"#\", \"Header_1\"),\n                (\"##\", \"Header_2\"),\n                (\"###\", \"Header_3\"),\n            ],\n        )\n        splits = [split for doc in docs for split in splitter.split_text(doc.page_content)]\n        print(f\"   Markdown splitting produced: {len(splits)} chunks\")\n    else:\n        raise ValueError(f\"Unexpected export type: {EXPORT_TYPE}\")\n    \n    # Step 3: Create embeddings and vector store\n    print(f\"\\n3. Creating embeddings with model: {EMBED_MODEL_ID}\")\n    embedding = HuggingFaceEmbeddings(model_name=EMBED_MODEL_ID)\n    \n    print(\"   Initializing Milvus vector store...\")\n    milvus_uri = str(Path(mkdtemp()) / \"docling.db\")\n    print(f\"   Vector DB location: {milvus_uri}\")\n    \n    vectorstore = Milvus.from_documents(\n        documents=splits,\n        embedding=embedding,\n        collection_name=\"docling_demo\",\n        connection_args={\"uri\": milvus_uri},\n        index_params={\"index_type\": \"FLAT\"},\n        drop_old=True,\n    )\n    print(f\"   Vector store created with {len(splits)} documents\")\n    \n    # Step 4: Create retrieval chain\n    print(f\"\\n4. Setting up RAG chain with LLM: {GEN_MODEL_ID}\")\n    print(f\"   Top-K retrieval: {TOP_K}\")\n    \n    retriever = vectorstore.as_retriever(search_kwargs={\"k\": TOP_K})\n    \n    llm = HuggingFaceEndpoint(\n        repo_id=GEN_MODEL_ID,\n        huggingfacehub_api_token=HF_TOKEN,\n    )\n    \n    question_answer_chain = create_stuff_documents_chain(llm, PROMPT)\n    rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n    \n    # Step 5: Query the RAG system\n    print(\"\\n5. Querying the RAG system...\")\n    print(f\"   Question: {QUESTION}\")\n    print(\"\\n\" + \"-\" * 80)\n    \n    resp_dict = rag_chain.invoke({\"input\": QUESTION})\n    \n    # Display results\n    clipped_answer = clip_text(resp_dict[\"answer\"], threshold=200)\n    print(f\"\\nQuestion:\\n{resp_dict['input']}\\n\")\n    print(f\"Answer:\\n{clipped_answer}\\n\")\n    \n    print(\"-\" * 80)\n    print(\"\\nRetrieved Sources:\")\n    print(\"-\" * 80)\n    \n    for i, doc in enumerate(resp_dict[\"context\"]):\n        print(f\"\\nSource {i + 1}:\")\n        print(f\"  text: {json.dumps(clip_text(doc.page_content, threshold=350))}\")\n        for key in doc.metadata:\n            if key != \"pk\":\n                val = doc.metadata.get(key)\n                clipped_val = clip_text(val) if isinstance(val, str) else val\n                print(f\"  {key}: {clipped_val}\")\n    \n    print(\"\\n\" + \"=\" * 80)\n    print(\"RAG pipeline completed successfully!\")\n    print(\"=\" * 80)\n\n\nif __name__ == \"__main__\":\n    main()","comments":[],"title":"RAG"}
